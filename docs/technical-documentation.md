# AlphaWave AI Assistant — Technical Documentation

## 1. System Overview

AlphaWave is a full-stack, self-hosted Retrieval-Augmented Generation (RAG) AI assistant platform. The system enables users to query an organization's private knowledge base via natural language and receive grounded, context-aware responses generated by a local Large Language Model (LLM).

The architecture follows a strict **client-server separation**:
- A **Python backend** handles all AI inference, document retrieval, and data ingestion.
- A **React frontend** provides the user interface via REST API calls.

---

## 2. Architecture Diagram

```
┌─────────────────────────────────────────────────────────────────┐
│                        FRONTEND (React/Vite)                    │
│                                                                 │
│   Landing Page  →  Floating Widget  ←→  Full Screen UI         │
│                          │                                      │
│              POST /chat  │  { question: string }               │
└──────────────────────────┼──────────────────────────────────────┘
                           │ HTTP (REST)
┌──────────────────────────▼──────────────────────────────────────┐
│                      BACKEND (FastAPI)                          │
│                           api.py                               │
└──────────────────────────┬──────────────────────────────────────┘
                           │
┌──────────────────────────▼──────────────────────────────────────┐
│                      RAG PIPELINE (rag.py)                      │
│                                                                 │
│  1. Embed question  →  2. Vector search  →  3. Build context    │
│                                                                 │
│  4. LangChain LCEL: PromptTemplate | ChatOllama | StrOutputParser│
└──────────┬──────────────────────────────────────┬──────────────┘
           │                                      │
┌──────────▼──────────┐               ┌───────────▼──────────────┐
│  Ollama (Local LLM) │               │  PostgreSQL 18 + pgvector │
│  Model: llama3       │               │  Docker Container         │
│  Embeddings:         │               │  Port: 5433               │
│  nomic-embed-text   │               │  DB: alphawave_ai         │
│  (768 dimensions)   │               │                           │
└──────────────────────┘               └───────────────────────────┘
```

---

## 3. Backend

### 3.1 `app/api.py` — REST API

The entry point for all client requests. Built with **FastAPI**.

**Endpoint:**

| Method | Path | Description |
|--------|------|-------------|
| `POST` | `/chat` | Accepts a question and returns an AI-generated answer |

**Request Body:**
```json
{ "question": "string" }
```

**Response:**
```json
{ "answer": "string" }
```

**CORS** is configured with `allow_origins=["*"]` for development. This should be restricted to the frontend domain in production.

---

### 3.2 `app/rag.py` — RAG Pipeline

The core intelligence layer. Uses **LangChain LCEL** (Expression Language) to define a composable chain.

**Chain Definition:**
```python
chain = PromptTemplate | ChatOllama(model="llama3") | StrOutputParser()
```

**Flow:**
1. Receive `question` string from the API.
2. Call `search_similar_documents(question, limit=10)` to retrieve the top 10 semantically similar document chunks from PostgreSQL.
3. Concatenate retrieved chunks into a `context` string.
4. Invoke the LCEL chain with `{context, question}`.
5. Return the LLM's plain string response.

**Prompt Template:**
```
You are an AI assistant for AlphaWave.
Answer the question using ONLY the context below.
If the answer is not contained in the context, say "I don't know."

Context: {context}
Question: {question}
Answer:
```

---

### 3.3 `app/database.py` — PostgreSQL & pgvector

Handles all database interactions via raw `psycopg2` SQL for full control over the `pgvector` query planner.

**Connection Config:**

| Parameter | Value |
|-----------|-------|
| Host | `localhost` |
| Port | `5433` |
| Database | `alphawave_ai` |
| User | `postgres` |
| Password | `postgres` |

**Schema (`documents` table):**

| Column | Type | Description |
|--------|------|-------------|
| `id` | `SERIAL PRIMARY KEY` | Auto-increment row ID |
| `url` | `TEXT` | Source URL of the document |
| `title` | `TEXT` | Page title / chunk label |
| `content` | `TEXT` | The raw text chunk |
| `embedding` | `VECTOR(768)` | 768-dim embedding from nomic-embed-text |

**Search Strategy — Hybrid:**

The `search_similar_documents()` function combines two retrieval signals:
1. **Keyword match**: Uses `ILIKE` against `title` and `content` for exact term presence.
2. **Vector similarity**: Uses the `<=>` (cosine distance) operator from pgvector.

Documents matching keywords are ranked first (`ORDER BY CASE ... END`), then ordered by cosine distance. This hybrid approach produces more relevant results than either method alone.

---

### 3.4 `app/embeddings.py` — Embedding Generation

Wraps LangChain's `OllamaEmbeddings` to generate vector representations of text.

```python
embeddings = OllamaEmbeddings(model="nomic-embed-text")
generate_embedding(text) → list[float]  # length: 768
```

Called by both `database.py` (during insert and search) and can be tested standalone.

---

### 3.5 `app/chunking.py` — Text Splitting

Wraps LangChain's `RecursiveCharacterTextSplitter` to split long documents into overlapping chunks before ingestion.

| Parameter | Value |
|-----------|-------|
| `chunk_size` | 400 characters |
| `chunk_overlap` | 100 characters |

The recursive splitter tries to split on paragraphs, then sentences, then words — preserving semantic coherence better than a simple character split.

---

### 3.6 `app/scraper.py` — Web Crawler & Data Ingestion

A custom BFS (breadth-first search) web crawler that scrapes an entire domain and populates the database.

**Process:**
1. Start from a `base_url` (e.g., `https://alphawave.hr/`).
2. Parse the page with **BeautifulSoup**, strip scripts/styles, extract visible text.
3. Discover and enqueue **internal links** (same-domain only).
4. For each page, chunk the text with `chunk_text()` and call `insert_document()` for each chunk.
5. After crawl, runs `ANALYZE documents;` to update PostgreSQL statistics for better query planning.

---

## 4. Frontend

### 4.1 Application Structure

All frontend code lives in `frontend/src/`. Entry point is `main.jsx` → renders `App.jsx`.

| File | Purpose |
|------|---------|
| `App.jsx` | Root component: navbar, hero section, state management for UI mode |
| `ChatWidget.jsx` | Floating popup chat — minimalist widget |
| `ChatWidget.css` | Styles for the popup widget |
| `FullChat.jsx` | Full-screen chat interface |
| `FullChat.css` | Styles for the full-screen UI |
| `App.css` | Global landing page styles |
| `index.css` | Base CSS reset and typography |

### 4.2 State Management (App.jsx)

```jsx
const [showFullChat, setShowFullChat] = useState(false);
```

- `showFullChat = false` → Landing page + `<ChatWidget />`
- `showFullChat = true` → `<FullChat onClose={() => setShowFullChat(false)} />`

The two chat modes are mutually exclusive. The widget is hidden when the full UI is active.

### 4.3 ChatWidget (Floating Popup)

- Fixed bottom-right position on desktop.
- On mobile (`< 480px`): popup becomes `100vw × 100vh` full-screen.
- **Enter** to send, **Shift+Enter** for new line.
- Auto-resizing textarea using `useEffect` + `scrollHeight`.
- Animated typing indicator (3 bouncing dots) while waiting for response.

### 4.4 FullChat (Full-Screen Interface)

- Overlays the entire viewport with `position: fixed`.
- Shares the same backend integration as the widget.
- Fade-in animation on open.
- `onClose` prop triggers a callback in `App.jsx` to return to the landing page.

### 4.5 Backend Communication

Both chat interfaces communicate with the backend via the **Fetch API**:

```javascript
const response = await fetch("http://127.0.0.1:8000/chat", {
  method: "POST",
  headers: { "Content-Type": "application/json" },
  body: JSON.stringify({ question: userMessage })
});
const data = await response.json(); // { answer: string }
```

---

## 5. Data Flow — Full Request Lifecycle

```
User types question → Fetch POST /chat
  → FastAPI receives QuestionRequest
    → rag.generate_answer(question)
      → embeddings.generate_embedding(question)         [OllamaEmbeddings → nomic-embed-text]
      → database.search_similar_documents(question)     [hybrid SQL: cosine + keyword]
      → Build context string from top 10 chunks
      → chain.invoke({context, question})               [LangChain LCEL]
        → PromptTemplate formats prompt
        → ChatOllama sends to llama3 via Ollama
        → StrOutputParser extracts string
      → return answer string
  → FastAPI returns { "answer": "..." }
User sees AI response in chat UI
```

---

## 6. Infrastructure

### Docker — PostgreSQL + pgvector

```bash
docker run --name alphawave-db \
  -e POSTGRES_DB=alphawave_ai \
  -e POSTGRES_USER=postgres \
  -e POSTGRES_PASSWORD=postgres \
  -p 5433:5432 \
  -d ankane/pgvector
```

The `pgvector` extension enables the `VECTOR` column type and the `<=>` cosine distance operator directly in SQL queries.

### Ollama — Local LLM Server

Ollama serves both models locally over HTTP. Required models:

```bash
ollama pull llama3           # Text generation
ollama pull nomic-embed-text # Embedding generation (768-dim)
```

---

## 7. Startup Order

1. Start Docker Desktop and ensure `alphawave-db` container is running.
2. Start Ollama.
3. Activate the Python virtual environment and start the backend:
   ```bash
   .\venv\Scripts\activate
   uvicorn app.api:app --reload
   ```
4. In a separate terminal, start the frontend:
   ```bash
   cd frontend
   npm run dev
   ```
5. Open `http://localhost:5173` in a browser.

---

## 8. Key Dependencies

### Python
| Package | Version | Purpose |
|---------|---------|---------|
| `fastapi` | latest | REST API framework |
| `uvicorn` | latest | ASGI server |
| `langchain` | 1.2.x | LLM orchestration |
| `langchain-core` | latest | Prompts, parsers, LCEL primitives |
| `langchain-ollama` | latest | ChatOllama, OllamaEmbeddings wrappers |
| `langchain-text-splitters` | latest | RecursiveCharacterTextSplitter |
| `psycopg2-binary` | latest | PostgreSQL driver |
| `beautifulsoup4` | latest | HTML parsing for scraper |
| `requests` | latest | HTTP client for scraper |
| `python-dotenv` | latest | Environment variable management |

### Node.js / Frontend
| Package | Version | Purpose |
|---------|---------|---------|
| `react` | ^19 | UI library |
| `react-dom` | ^19 | DOM renderer |
| `vite` | ^7 | Build tool and dev server |

---

## 9. Security Considerations

- **Local-only architecture**: All LLM inference and data storage occur on-premise. No data is sent to external APIs.
- **CORS**: Currently set to `allow_origins=["*"]`. Must be restricted to the frontend origin in production.
- **Database credentials**: Currently hardcoded in `database.py`. Should be moved to a `.env` file loaded via `python-dotenv` before any public deployment.
- **Ollama default port**: `11434` — not exposed externally; only the backend communicates with it locally.
